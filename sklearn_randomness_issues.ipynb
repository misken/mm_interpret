{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a0d8d-c2ee-4ad7-bb34-49f65c721a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b21b77-c1cc-4b71-8e66-7997fbc7139e",
   "metadata": {},
   "source": [
    "### Old analysis related to forgetting to pass cv to cross_val_predict\n",
    "\n",
    "> I'm keeping this around because it's got some good stuff related to manually exploring the folds in k-crossfold and general advice on `cross_validate`, `cross_val_score`, and `cross_val_predict`.\n",
    "\n",
    "\n",
    "Interestingly, the noq fits better than basicq. Is this overfitting? But, looking at metrics df's, basicq fits better. Is this possible? \n",
    "\n",
    "Yes, seems like this is related to difference between `cross_val_score` (equivalently, `cross_validate` which I use so that I can use multiple scoring metrics) and `cross_val_predict`. See, for example, https://stackoverflow.com/questions/55009704/why-is-cross-val-predict-not-appropriate-for-measuring-the-generalisation-error. This is also discussed in the sklearn docs. In a nutshell, `cross_validate` reports scores averaged over folds and `cross_val_predict` simply provides prediction for each data point when it was in the test set. Computing the error metric based on these predictions and the actuals does not necessarily equate to the averages over folds (measures might not be linear in sense that average of averages is not same as average over entire set).\n",
    "\n",
    "**HOWEVER, I still don't see how it's possible to get the two plots above and yet in the `metrics_df`'s below, the basicq had lower MAEs in all folds than noq.**\n",
    "\n",
    "**2021-10-01 - I THINK I FOUND THE PROBLEM! I was not passing cv=cv_iterator into cross_val_predict which means the folds in cross_validate were not the same as those in cross_val_predict. Doh!**\n",
    "\n",
    "**YES, that was the issue. Of course, that means I need to rerun all the model fits yet again.**\n",
    "\n",
    "From the SO post, both are reasonable ways of looking at relative model performance but if one really wants to get at generalization error, probably need to have a leave out test set that's not used at all in CV procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34184c79-4fa3-40ad-a00b-2970d86aee53",
   "metadata": {},
   "source": [
    "It's possible to get the kfold indices to more deeply explore model performance. \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef38f2-eeda-4605-bb11-7eae2db43aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pp_noq = pd.read_csv('data/X_pp_noq_exp11.csv', index_col=0)\n",
    "X_pp_basicq = pd.read_csv('data/X_pp_basicq_exp11.csv', index_col=0)\n",
    "y_pp_occ_p95 = pd.read_csv('data/y_pp_occ_p95_exp11.csv', index_col=0, squeeze=True)\n",
    "y_pp_occ_p95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6043d4-e9b8-4c38-b2a6-3600b4cf1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d2022-29f5-48f2-82a4-37058ff34f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf0089-5ee1-4b55-a433-19144353da1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cv = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "split=1\n",
    "for train_index, test_index in cv.split(X_pp_noq):\n",
    "    print(f\"Split {split}\")\n",
    "    print(train_index, test_index)\n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfd9ed-5400-4dfa-857d-820510fc827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "steps = []\n",
    "steps.extend([PolynomialFeatures(2), LinearRegression(fit_intercept=True)])\n",
    "model = make_pipeline(*steps)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "split = 1\n",
    "noq_data = {}\n",
    "for train_index, test_index in cv.split(X_pp_noq):\n",
    "    data = {}\n",
    "    X_train = X_pp_noq.iloc[train_index]\n",
    "    X_test = X_pp_noq.iloc[test_index]\n",
    "    y_train = y_pp_occ_p95.iloc[train_index]\n",
    "    y_test = y_pp_occ_p95.iloc[test_index]\n",
    "    print(f\"Split {split}\")\n",
    "    # Fit on train\n",
    "    model.fit(X_train, y_train)\n",
    "    y_fitted = model.predict(X_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_fitted)\n",
    "    mae_test = mean_absolute_error(y_test, y_predict)\n",
    "    print(f\"mae_train: {mae_train}, mae_test: {mae_test}\")\n",
    "    \n",
    "    data['train_index'] = train_index\n",
    "    data['test_index'] = test_index\n",
    "    data['y_train'] = y_train\n",
    "    data['y_test'] = y_test\n",
    "    data['y_fitted'] = y_fitted\n",
    "    data['y_predict'] = y_predict\n",
    "    \n",
    "    noq_data[split] = data\n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38da0f-c585-4437-9825-0715e9d2f247",
   "metadata": {},
   "source": [
    "Yep, these match the metrics_df shown below. Now let's do basicq. Yes, they match too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5409a-6d07-48ae-9772-3e6c642e0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "steps = []\n",
    "steps.extend([PolynomialFeatures(2), LinearRegression(fit_intercept=True)])\n",
    "model = make_pipeline(*steps)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "split = 1\n",
    "basicq_data = {}\n",
    "for train_index, test_index in cv.split(X_pp_basicq):\n",
    "    data = {}\n",
    "    X_train = X_pp_basicq.iloc[train_index]\n",
    "    X_test = X_pp_basicq.iloc[test_index]\n",
    "    y_train = y_pp_occ_p95.iloc[train_index]\n",
    "    y_test = y_pp_occ_p95.iloc[test_index]\n",
    "    print(f\"Split {split}\")\n",
    "    \n",
    "    # Fit on train\n",
    "    model.fit(X_train, y_train)\n",
    "    y_fitted = model.predict(X_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_fitted)\n",
    "    mae_test = mean_absolute_error(y_test, y_predict)\n",
    "    print(f\"mae_train: {mae_train}, mae_test: {mae_test}\")\n",
    "    \n",
    "    data['train_index'] = train_index\n",
    "    data['test_index'] = test_index\n",
    "    data['y_train'] = y_train\n",
    "    data['y_test'] = y_test\n",
    "    data['y_fitted'] = y_fitted\n",
    "    data['y_predict'] = y_predict\n",
    "    \n",
    "    basicq_data[split] = data\n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef2043-25af-43ed-b2c7-0891bb5b0b85",
   "metadata": {},
   "source": [
    "Ok, how to look at each of the folds and try to make sense of how the overall graphs can suggest noq fits better than basicq while the metrics_df summaries suggest the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db1a49-246a-4c96-ae5e-643a4b375f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 4\n",
    "actual = noq_data[fold]['y_test']\n",
    "noq_pred = noq_data[fold]['y_predict']\n",
    "basicq_pred = basicq_data[fold]['y_predict']\n",
    "\n",
    "mae_noq = mean_absolute_error(actual, noq_pred)\n",
    "mae_basicq = mean_absolute_error(actual, basicq_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280d7e1-7300-4a08-963e-60772e802215",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(1, 6):\n",
    "    print(basicq_data[fold]['y_predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7ba64-896e-4097-94c1-a25dda90f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_anchor=0\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(10,10))\n",
    "ax[0].scatter(actual, noq_pred, color='red')\n",
    "ax[0].set_title('noq')\n",
    "ax[0].annotate(f\"{mae_noq:.3f}\", xy=(5, 50),  xycoords='data')\n",
    "ax[1].scatter(actual, basicq_pred, color='blue')\n",
    "ax[1].set_title('basicq')\n",
    "ax[1].annotate(f\"{mae_basicq:.3f}\", xy=(5, 50),  xycoords='data')\n",
    "for axis in ax:\n",
    "    axis.axline((ax_anchor - 0.1 * ax_anchor, ax_anchor - 0.1 * ax_anchor), slope=1)\n",
    "    axis.set_xlabel('actual')  # Add an x-label to the axes.\n",
    "    axis.set_ylabel('predicted')  # Add a y-label to the axes.\n",
    "    axis.set_xlim(0, 80)\n",
    "    axis.set_ylim(0, 80)\n",
    "fig.tight_layout()\n",
    "#ax.set_title(title)  # Add a title to the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fcc496-ded0-4d72-9d65-331eca0651a4",
   "metadata": {},
   "source": [
    "The individual fold scatter plots and examination of the test predictions do not indicate the points appearing in the original scatter (e.g. actual ~ 10 and pred ~ 15)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155d4c0-02ea-47ce-a29d-ec4085ade872",
   "metadata": {},
   "source": [
    "#### Another issue related to randomness\n",
    "https://scikit-learn.org/stable/common_pitfalls.html#randomness\n",
    "\n",
    "In researching the details of `cross_validate` and `cross_val_predict`, I uncovered another potential complication related to integer random number seeds vs `RandomState` instances. See the link above for the example of how in a RF, an integer seed will then lead to CRN in the random parts of RF. This isn't necessarily terrible.\n",
    "\n",
    "I've got an input param in `crossval_summarize_mm` for the random forest random state. Right now it's set to 0 (an int) but I could change this to a `RandomState`. I don't this is a big deal in this study.\n",
    "\n",
    "It might also be useful to rerun the experiments with a different random int for my `kfold_random_state` param in ``crossval_summarize_mm` (currently defaulted to 4)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analytics]",
   "language": "python",
   "name": "conda-env-analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
